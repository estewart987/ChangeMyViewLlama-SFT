import torch
import os
import matplotlib.pyplot as plt
import torch.nn as nn
from tqdm import tqdm
from openai import OpenAI
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, LlamaForCausalLM

from fine_tune_model import generate_response


def load_model_and_tokenizer(model_save_path: str) -> tuple[LlamaForCausalLM, AutoTokenizer]:
    """
    Load the fine-tuned model and tokenizer from the specified directory.

    Args:
        model_save_path (str): The directory where the model and tokenizer are saved.

    Returns:
        model (LlamaForCausalLM): The loaded model.
        tokenizer (AutoTokenizer): The loaded tokenizer.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_save_path)
    model = LlamaForCausalLM.from_pretrained(model_save_path)
    return model, tokenizer

# Evaluate responses using GPT 4o-mini
def gpt4o_eval(
        input: str,
        generated_response: str,
        target_response: str,
        model: str = "gpt-4o-mini",
        max_tokens: int = 100,
        temperature: float = 0.5
    ) -> dict:
    """
    Evaluate a generated response using the GPT-4o-mini model.

    Args:
        input (str): The input given to the model.
        generated_response (str): The response generated by the model.
        target_response (str): The target response.
        model (str): The model to use for evaluation.
        max_tokens (int): The maximum number of tokens to generate.
        temperature (float): The temperature for sampling.

    Returns:
        dict: The evaluation response from the GPT-4o-mini model.
    """

    eval_prompt = f"""
        This was the input given to the model:
        "Below is a post from the Change my Views subreddit. Write a response that you think will change the author's view.\n {input}"
        Evaluate the following response generated by a model:
        Generated Response: "{generated_response}"
        Target Response: "{target_response}"

        Provide a score from 1 to 10 for the following criteria:
        1. Relevance: How closely the response aligns with the target response.
        2. Coherence: How logically and fluently the response is structured.
        3. Completeness: Does the response cover all aspects of the target response?

        Also, provide a brief justification for the scores.
        """
    api_key = os.getenv("OPENAI_API_KEY")
    client = OpenAI(api_key = api_key)
    chat_completion = client.chat.completions.create(
        messages = [
            {"role":"user",
             "content":eval_prompt}
        ],
        model=model,
        max_tokens=max_tokens,
        temperature=temperature
    )

    return chat_completion.choices[0].message.content

def evaluate_with_dataloader(dataset, trained_model, tokenizer, eval_model, device, batch_size, num_workers=0):
    """
    Evaluate the model using the test dataset.

    Args:
        dataset (Dataset): The test dataset.
        trained_model (LlamaForCausalLM): The trained model.
        tokenizer (AutoTokenizer): The tokenizer.
        eval_model (str): The evaluation model.
        device (str): The device to use for evaluation.
        batch_size (int): The batch size for evaluation.
        num_workers (int): The number of workers for the DataLoader.

    Returns:
        tuple: A tuple containing the average relevance, coherence, and completeness scores.
    """
    dataloader = DataLoader(dataset, tokenizer, batch_size, num_workers, shuffle=False, drop_last=False)
    relevance_scores, coherence_scores, completeness_scores = [], [], []

    for batch in tqdm(dataloader, desc="Evaluating test set"):
        for i in range(len(batch["prompt_tokens"])):
            input_tokens = batch["prompt_tokens"][i].to(device)
            target_tokens = batch["target_tokens"][i].to(device)

            # Decode input and target for GPT-4o-mini evaluation
            input_text = tokenizer.decode(input_tokens, skip_special_tokens=True)
            target_response = tokenizer.decode(target_tokens, skip_special_tokens=True)

            # Generate response from Llama
            generated_response = generate_response(
                model=trained_model, 
                tokenizer=tokenizer, 
                prompt=input_text, 
                device=device
            )

            # Evaluate using GPT-4o-mini
            evaluation = gpt4o_eval(
                input=input_text,
                generated_response=generated_response,
                target_response=target_response,
                model=eval_model
            )

            # Parse and append scores
            try:
                relevance = float(evaluation.split("Relevance:")[1].split("\n")[0].strip())
                coherence = float(evaluation.split("Coherence:")[1].split("\n")[0].strip())
                completeness = float(evaluation.split("Completeness:")[1].split("\n")[0].strip())
                relevance_scores.append(relevance)
                coherence_scores.append(coherence)
                completeness_scores.append(completeness)
            except (IndexError, ValueError):
                print("Failed to parse GPT-4o-mini response:", evaluation)
                continue

    # Calculate averages
    avg_relevance = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0
    avg_coherence = sum(coherence_scores) / len(coherence_scores) if coherence_scores else 0
    avg_completeness = sum(completeness_scores) / len(completeness_scores) if completeness_scores else 0

    print("\n--- Average Evaluation Scores ---")
    print(f"Relevance: {avg_relevance:.2f}")
    print(f"Coherence: {avg_coherence:.2f}")
    print(f"Completeness: {avg_completeness:.2f}")

    return avg_relevance, avg_coherence, avg_completeness

if __name__ == "__main__":
    test_dataset = torch.load("test_dataset.pth")

    # Load in fine-tuned model
    model_save_path = "./fine_tuned_llama_model"
    model, tokenizer = load_model_and_tokenizer(model_save_path)

    device = torch.device("mps")

    avg_scores = evaluate_with_dataloader(test_dataset, model, tokenizer, eval_model="gpt-4o-mini", device=device, batch_size=1)